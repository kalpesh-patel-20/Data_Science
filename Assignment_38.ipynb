{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6dsG4fmV-rM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1. What is the Filter method in feature selection, and how does it work?**\n",
        "\n",
        "**Answer:**  \n",
        "The **Filter method** selects features based on statistical tests or scoring functions before any machine learning model is applied. It ranks features based on metrics like:\n",
        "- Correlation with the target variable\n",
        "- Chi-square test\n",
        "- Mutual information\n",
        "- ANOVA F-score\n",
        "\n",
        "**Example:**  \n",
        "Using `SelectKBest` with `f_classif` to select top features in scikit-learn.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q2. How does the Wrapper method differ from the Filter method in feature selection?**\n",
        "\n",
        "**Answer:**  \n",
        "- The **Wrapper method** uses a predictive model to evaluate the performance of different subsets of features. It is model-specific and searches for the best subset by training and testing the model on various combinations.\n",
        "- The **Filter method** is model-agnostic and selects features based on intrinsic data properties.\n",
        "\n",
        "**Wrapper = Higher accuracy but slower**  \n",
        "**Filter = Faster but may ignore feature interactions**\n",
        "\n",
        "---\n",
        "\n",
        "### **Q3. What are some common techniques used in Embedded feature selection methods?**\n",
        "\n",
        "**Answer:**\n",
        "Embedded methods integrate feature selection during model training. Common techniques:\n",
        "- **L1 Regularization (Lasso)**: Shrinks some coefficients to zero, performing automatic feature selection.\n",
        "- **Decision Tree-based methods**: Feature importance can be extracted during training.\n",
        "- **Elastic Net**: Combines L1 and L2 penalties for more balanced selection.\n",
        "- **RFE with embedded models**: Recursive Feature Elimination using embedded estimators.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q4. What are some drawbacks of using the Filter method for feature selection?**\n",
        "\n",
        "**Answer:**\n",
        "- Ignores feature interactions (considers features independently)\n",
        "- May select irrelevant features if the scoring metric is not appropriate\n",
        "- Not tailored to specific model performance\n",
        "- Risk of discarding useful features if their individual score is low\n",
        "\n",
        "---\n",
        "\n",
        "### **Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature selection?**\n",
        "\n",
        "**Answer:**\n",
        "- When you have **a large number of features** (high-dimensional data)\n",
        "- When **computation speed** is crucial\n",
        "- When you're performing **preliminary feature selection**\n",
        "- For **unsupervised learning** or **preprocessing before training**\n",
        "\n",
        "---\n",
        "\n",
        "### **Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn. You are unsure of which features to include in the model because the dataset contains several different ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.**\n",
        "\n",
        "**Answer:**  \n",
        "- Start by calculating **correlation coefficients** or **mutual information** between each feature and the churn label.\n",
        "- Use `SelectKBest` or `VarianceThreshold` to select features with strong relationships.\n",
        "- Drop features with low variance or high correlation with other features (multicollinearity).\n",
        "- Rank the features and select the top K based on the score.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with many features, including player statistics and team rankings. Explain how you would use the Embedded method to select the most relevant features for the model.**\n",
        "\n",
        "**Answer:**  \n",
        "- Train a **Lasso Logistic Regression model** on the dataset. The L1 penalty will shrink coefficients of less important features to zero.\n",
        "- Alternatively, use a **Random Forest Classifier** and extract `feature_importances_`.\n",
        "- Rank features by importance and keep the top N for final model training.\n",
        "- Benefit: Feature selection is integrated into the training process, maintaining interaction information.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q8. You are working on a project to predict the price of a house based on its features, such as size, location, and age. You have a limited number of features, and you want to ensure that you select the most important ones for the model. Explain how you would use the Wrapper method to select the best set of features for the predictor.**\n",
        "\n",
        "**Answer:**  \n",
        "- Choose a **base model** like Linear Regression or Decision Tree.\n",
        "- Use **Recursive Feature Elimination (RFE)** or **Sequential Feature Selection** to evaluate different subsets of features by training the model multiple times.\n",
        "- Select the subset that gives the best cross-validated score.\n",
        "- Since the number of features is small, computational cost is manageable, and Wrapper can provide optimal performance.\n"
      ],
      "metadata": {
        "id": "RLRrToOSWBRA"
      }
    }
  ]
}