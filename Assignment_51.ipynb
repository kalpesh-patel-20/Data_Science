{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate.**\n",
        "\n",
        "- **Linear Regression:**\n",
        "  - Predicts **continuous values**.\n",
        "  - Uses a linear equation: `y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô`.\n",
        "  - Suitable for tasks like predicting house prices, temperature, or stock prices.\n",
        "\n",
        "- **Logistic Regression:**\n",
        "  - Predicts **probabilities** for classification problems.\n",
        "  - Uses the **sigmoid (logistic) function** to map output to a value between 0 and 1.\n",
        "  - Suitable for binary/multiclass classification tasks.\n",
        "\n",
        "üîç **Example:**  \n",
        "Predicting whether an email is spam (`1`) or not spam (`0`). This is a binary classification problem, so **logistic regression** is more appropriate.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q2. What is the cost function used in logistic regression, and how is it optimized?**\n",
        "\n",
        "- **Cost Function:**  \n",
        "  Logistic regression uses the **log loss** or **binary cross-entropy loss**:\n",
        "\n",
        " $$   J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))]\n",
        "  $$\n",
        "\n",
        "- **Optimization:**\n",
        "  - Typically optimized using **Gradient Descent**, which iteratively updates the weights to minimize the loss.\n",
        "  - Modern implementations may use **advanced optimizers** like Adam, RMSProp, or L-BFGS.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.**\n",
        "\n",
        "- **Regularization:**  \n",
        "  A technique to **penalize complex models** by adding a penalty term to the cost function.\n",
        "\n",
        "- **Types:**\n",
        "  - **L1 Regularization (Lasso):** Adds `Œª * ||Œ∏||‚ÇÅ` to the cost function; helps in feature selection by driving some weights to zero.\n",
        "  - **L2 Regularization (Ridge):** Adds `Œª * ||Œ∏||‚ÇÇ¬≤` to the cost function; helps reduce model complexity without eliminating features.\n",
        "\n",
        "- **Benefit:**  \n",
        "  Prevents the model from fitting noise in the training data (overfitting) and improves generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?**\n",
        "\n",
        "- **ROC Curve (Receiver Operating Characteristic):**\n",
        "  - Plots **True Positive Rate (Recall)** against **False Positive Rate** at various threshold levels.\n",
        "\n",
        "- **Purpose:**\n",
        "  - Evaluates how well a classification model distinguishes between classes.\n",
        "  - **AUC (Area Under Curve):** A value closer to 1.0 indicates a better performing model.\n",
        "\n",
        "üîç **Use Case:**  \n",
        "Helps choose an optimal classification threshold and compare models across imbalanced datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?**\n",
        "\n",
        "**Feature Selection Techniques:**\n",
        "1. **Univariate Selection (e.g., chi-square test)**\n",
        "2. **Recursive Feature Elimination (RFE)**\n",
        "3. **L1 Regularization (Lasso)**\n",
        "4. **Tree-based Feature Importance**\n",
        "5. **Correlation Matrix Analysis**\n",
        "\n",
        "‚úÖ **Benefits:**\n",
        "- Reduces overfitting.\n",
        "- Improves model interpretability.\n",
        "- Lowers training time and computational cost.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?**\n",
        "\n",
        "**Techniques:**\n",
        "1. **Resampling:**\n",
        "   - **Oversampling** (e.g., SMOTE)\n",
        "   - **Undersampling** the majority class\n",
        "2. **Class Weights:**\n",
        "   - Adjust class weights in logistic regression (`class_weight='balanced'` in scikit-learn)\n",
        "3. **Threshold Tuning:**\n",
        "   - Shift the decision threshold to favor the minority class\n",
        "4. **Evaluation Metrics:**\n",
        "   - Use precision, recall, F1-score, ROC-AUC instead of accuracy\n",
        "\n",
        "---\n",
        "\n",
        "### **Q7. Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?**\n",
        "\n",
        "**Common Challenges:**\n",
        "\n",
        "1. **Multicollinearity:**\n",
        "   - When predictors are highly correlated, it inflates variance and affects coefficient estimates.\n",
        "   - **Solutions:**\n",
        "     - Remove correlated features (using correlation matrix or VIF).\n",
        "     - Use **Principal Component Analysis (PCA)** to reduce dimensionality.\n",
        "     - Apply **L2 regularization** to reduce the effect of multicollinearity.\n",
        "\n",
        "2. **Non-linearly Separable Data:**\n",
        "   - Logistic regression assumes linear decision boundary.\n",
        "   - **Solution:** Use polynomial features or switch to non-linear models (e.g., SVM with kernel).\n",
        "\n",
        "3. **Imbalanced Dataset:**\n",
        "   - Model becomes biased toward the majority class.\n",
        "   - **Solution:** Use techniques from Q6.\n",
        "\n",
        "4. **Outliers:**\n",
        "   - Can skew model predictions.\n",
        "   - **Solution:** Remove or transform outliers, or use robust methods.\n",
        "\n",
        "5. **Large Feature Space:**\n",
        "   - Increases risk of overfitting.\n",
        "   - **Solution:** Feature selection, dimensionality reduction, and regularization.\n"
      ],
      "metadata": {
        "id": "s3w21MvF_TQZ"
      }
    }
  ]
}