{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHrWyet-5DEw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1. What is Ridge Regression, and how does it differ from Ordinary Least Squares (OLS) Regression?**\n",
        "\n",
        "**Ridge Regression** is a regularized version of linear regression that adds a penalty (called **L2 regularization**) to the loss function.  \n",
        "This penalty term shrinks the coefficients to prevent overfitting, especially when predictors are highly correlated.\n",
        "\n",
        "**OLS Regression** minimizes the sum of squared residuals.  \n",
        "**Ridge Regression** minimizes:  \n",
        "`Sum of Squared Errors + λ * Sum of Squares of Coefficients`\n",
        "\n",
        "➡️ **Key difference**: Ridge includes a penalty term controlled by **lambda (λ)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q2. What are the assumptions of Ridge Regression?**\n",
        "\n",
        "Ridge shares most assumptions with OLS regression:\n",
        "1. **Linearity** – The relationship between features and target is linear.\n",
        "2. **Independence** – Observations are independent of each other.\n",
        "3. **Homoscedasticity** – Constant variance of residuals.\n",
        "4. **No perfect multicollinearity** – Ridge helps *reduce* multicollinearity, but it still assumes it's not perfect.\n",
        "5. **Normality of errors** – Not strictly required but useful for inference.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?**\n",
        "\n",
        "Use **cross-validation** (like K-Fold CV) to find the best lambda (α in `Ridge(alpha=...)` in Python).\n",
        "\n",
        "Common approaches:\n",
        "- **Grid Search**: Try multiple values and pick the one with lowest error.\n",
        "- **Built-in functions**: `RidgeCV` in scikit-learn automatically selects the best lambda using CV.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q4. Can Ridge Regression be used for feature selection?**\n",
        "\n",
        "Not directly.\n",
        "\n",
        "- **Ridge shrinks** coefficients close to zero but **not exactly zero**.\n",
        "- It **keeps all features**, unlike **Lasso**, which can zero out coefficients (i.e., remove features).\n",
        "\n",
        "So Ridge is **not ideal** for feature selection but great for reducing model complexity.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q5. How does Ridge Regression perform with multicollinearity?**\n",
        "\n",
        "Ridge is **especially useful** in the presence of multicollinearity.\n",
        "\n",
        "- It **stabilizes** the regression coefficients by shrinking them.\n",
        "- Helps prevent extreme values due to high correlation among features.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q6. Can Ridge Regression handle both categorical and continuous independent variables?**\n",
        "\n",
        "Yes, but:\n",
        "- **Categorical variables** must be **converted** (e.g., using One-Hot Encoding).\n",
        "- Ridge works only with **numerical** data, so preprocessing is necessary.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q7. How do you interpret the coefficients of Ridge Regression?**\n",
        "\n",
        "Interpretation is similar to linear regression:\n",
        "- Each coefficient shows the **expected change** in the output for a one-unit change in that variable (holding others constant).\n",
        "\n",
        "However:\n",
        "- Due to regularization, coefficients are **shrunk**, so they may be **less interpretable**.\n",
        "- They are biased but have **lower variance**, which can improve prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?**\n",
        "\n",
        "Yes, but with care.\n",
        "\n",
        "- You must handle **time dependencies** using lag features, rolling averages, etc.\n",
        "- Use **time-based validation** (not random splits) when tuning lambda.\n",
        "\n",
        "➡️ Ridge helps if your time-series model has **many features or collinearity** among lagged variables."
      ],
      "metadata": {
        "id": "_LEcuRte5Fqv"
      }
    }
  ]
}