{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1. What is the purpose of Grid Search CV in machine learning, and how does it work?**\n",
        "\n",
        "**Purpose:**  \n",
        "Grid Search CV (Cross-Validation) is used to **tune hyperparameters** of a machine learning model to find the combination that yields the best performance.\n",
        "\n",
        "**How it works:**\n",
        "- You define a **grid of hyperparameters** and their possible values.\n",
        "- The model is trained and evaluated using **cross-validation** for **each combination**.\n",
        "- The combination with the best evaluation score is selected.\n",
        "\n",
        "üîç **Example:**  \n",
        "For an SVM model, you can tune `C`, `kernel`, and `gamma` using Grid Search CV.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q2. Describe the difference between Grid Search CV and Randomized Search CV, and when might you choose one over the other?**\n",
        "\n",
        "| Feature                  | Grid Search CV                         | Randomized Search CV                     |\n",
        "|--------------------------|----------------------------------------|------------------------------------------|\n",
        "| Search Method            | Exhaustively tries all combinations    | Randomly samples combinations            |\n",
        "| Time Complexity          | High (if grid is large)                | Lower (can specify number of iterations) |\n",
        "| When to Use              | Small search space                     | Large or complex search space            |\n",
        "\n",
        "‚úÖ **Choose Randomized Search** when:\n",
        "- You have **limited computational resources**.\n",
        "- The **parameter space is large** or continuous.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.**\n",
        "\n",
        "**Data Leakage:**  \n",
        "When **information from outside the training dataset** is used to create the model, leading to overly optimistic performance.\n",
        "\n",
        "**Why it‚Äôs a problem:**  \n",
        "It causes **unrealistically high accuracy** during training and poor **generalization to new data**.\n",
        "\n",
        "üîç **Example:**  \n",
        "If a feature in the dataset is a **future value** (like target sales next month), and it's used during training to predict current sales ‚Äî that‚Äôs data leakage.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q4. How can you prevent data leakage when building a machine learning model?**\n",
        "\n",
        "‚úÖ **Prevention Techniques:**\n",
        "1. **Separate preprocessing**: Always apply transformations (e.g., scaling, imputation) **after** splitting into train/test.\n",
        "2. **Avoid using future data** for training.\n",
        "3. **Be cautious with derived features** (ensure they don‚Äôt encode the target).\n",
        "4. Use **pipelines** (e.g., `sklearn.pipeline`) to ensure no leakage during preprocessing.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?**\n",
        "\n",
        "A **confusion matrix** is a table used to evaluate the performance of a classification algorithm.\n",
        "\n",
        "|                      | **Predicted Positive** | **Predicted Negative** |\n",
        "|----------------------|------------------------|------------------------|\n",
        "| **Actual Positive**  | True Positive (TP)     | False Negative (FN)    |\n",
        "| **Actual Negative**  | False Positive (FP)    | True Negative (TN)     |\n",
        "\n",
        "It gives detailed insight into **correct** and **incorrect** predictions, which is crucial beyond simple accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q6. Explain the difference between precision and recall in the context of a confusion matrix.**\n",
        "\n",
        "- **Precision** = TP / (TP + FP)  \n",
        "  ‚Üí Of all predicted positives, how many are truly positive?\n",
        "\n",
        "- **Recall** = TP / (TP + FN)  \n",
        "  ‚Üí Of all actual positives, how many did we correctly predict?\n",
        "\n",
        "üîç Example:  \n",
        "In a cancer detection model:\n",
        "- **High precision** = few false alarms.\n",
        "- **High recall** = few missed actual cancer cases.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?**\n",
        "\n",
        "- **High FP (False Positives):** Model is over-predicting the positive class.\n",
        "- **High FN (False Negatives):** Model is missing actual positive instances.\n",
        "- Use this to **analyze trade-offs** and tune model accordingly (e.g., changing threshold, balancing classes).\n",
        "\n",
        "---\n",
        "\n",
        "### **Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?**\n",
        "\n",
        "1. **Accuracy:**  \n",
        "   $$\n",
        "   \\frac{TP + TN}{TP + FP + FN + TN}\n",
        "   $$\n",
        "\n",
        "2. **Precision:**  \n",
        "   $$\n",
        "   \\frac{TP}{TP + FP}\n",
        "   $$\n",
        "\n",
        "3. **Recall (Sensitivity):**  \n",
        "   $$\n",
        "   \\frac{TP}{TP + FN}\n",
        "   $$\n",
        "\n",
        "4. **Specificity:**  \n",
        "   $$\n",
        "   \\frac{TN}{TN + FP}\n",
        "   $$\n",
        "\n",
        "5. **F1 Score (harmonic mean of precision & recall):**  \n",
        "   $$\n",
        "   2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "### **Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?**\n",
        "\n",
        "- **Accuracy** is directly computed from the confusion matrix:\n",
        "  \\[\n",
        "  \\text{Accuracy} = \\frac{TP + TN}{Total}\n",
        "  \\]\n",
        "\n",
        "However, in **imbalanced datasets**, a high accuracy can be **misleading**. Example:  \n",
        "If 95% of the data is negative, predicting all negatives gives 95% accuracy but fails completely at identifying positives.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?**\n",
        "\n",
        "‚úÖ **Use Cases:**\n",
        "- **Bias toward majority class:** Many FNs or FPs for minority class.\n",
        "- **Unbalanced predictions:** Model predicts only one class frequently.\n",
        "- **Systematic errors:** Repeated errors in a specific class indicate **label imbalance**, poor features, or data quality issues.\n",
        "\n",
        "‚û°Ô∏è Helps you fine-tune preprocessing, sampling strategies, or even the loss function to **mitigate bias and improve fairness**.\n"
      ],
      "metadata": {
        "id": "zFZm-4ttAUsk"
      }
    }
  ]
}