{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bk-uMgXBXD32"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. What is Min-Max Scaling?\n",
        "\n",
        "Min-Max scaling transforms features to a specific range, usually [0, 1].\n",
        "\n",
        "The formula is:\n",
        "\n",
        "$$\n",
        "X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
        "$$\n",
        "\n",
        "### ðŸ”¹ Example:\n",
        "\n",
        "Original data: `[10, 20, 30]`  \n",
        "Min = 10, Max = 30\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[10], [20], [30]])\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(\"Min-Max Scaled Data:\\n\", scaled_data)\n",
        "```\n",
        "---\n",
        "\n",
        "## Q2. What is the Unit Vector (Normalization) technique?\n",
        "\n",
        "This technique scales each data sample (vector) to have a unit norm (length = 1):\n",
        "\n",
        "$$\n",
        "x_{\\text{norm}} = \\frac{x}{\\|x\\|}\n",
        "$$\n",
        "\n",
        "### ðŸ”¹ Example:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "data = np.array([[3, 4]])\n",
        "normalizer = Normalizer()\n",
        "normalized_data = normalizer.fit_transform(data)\n",
        "print(\"Unit Vector Scaled (Normalized):\\n\", normalized_data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Q3. What is PCA (Principal Component Analysis)?\n",
        "\n",
        "**PCA** reduces dimensionality by transforming features into uncorrelated principal components that retain most variance.\n",
        "\n",
        "---\n",
        "\n",
        "## Q4. How does PCA relate to Feature Extraction?\n",
        "\n",
        "PCA is a **feature extraction** technique because it creates new features (principal components) as combinations of the original ones.\n",
        "\n",
        "---\n",
        "\n",
        "## Q5. Applying Min-Max Scaling to Food Delivery Data\n",
        "\n",
        "Assuming features like price, rating, and delivery time:\n",
        "\n",
        "```python\n",
        "data = np.array([\n",
        "    [300, 4.2, 30],\n",
        "    [500, 4.8, 45],\n",
        "    [150, 3.5, 20]\n",
        "])\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(data)\n",
        "print(\"Scaled Food Delivery Data:\\n\", scaled)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Q6. Using PCA for Stock Price Prediction\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Simulated high-dimensional data\n",
        "X = np.random.rand(100, 10)  # 100 samples, 10 features\n",
        "\n",
        "# Standardize before PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=0.95)  # Retain 95% variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"Original Shape: {X.shape}, After PCA: {X_pca.shape}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Q7. Min-Max Scaling to [-1, 1] for [1, 5, 10, 15, 20]\n",
        "\n",
        "Use formula:\n",
        "\n",
        "$$\n",
        "X' = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\cdot (b - a) + a\n",
        "$$\n",
        "\n",
        "where \\( a = -1 \\), \\( b = 1 \\)\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = np.array([[1], [5], [10], [15], [20]])\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(\"Scaled to [-1, 1]:\\n\", scaled_data.ravel())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Q8. PCA on [height, weight, age, gender, blood pressure]\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Simulated data\n",
        "df = pd.DataFrame({\n",
        "    'height': np.random.normal(170, 10, 100),\n",
        "    'weight': np.random.normal(70, 15, 100),\n",
        "    'age': np.random.randint(18, 65, 100),\n",
        "    'gender': np.random.choice([0, 1], 100),  # 0=Male, 1=Female\n",
        "    'bp': np.random.normal(120, 15, 100)\n",
        "})\n",
        "\n",
        "# Standardize\n",
        "scaled = StandardScaler().fit_transform(df)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(scaled)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Scree Plot')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Choose the number of components** where the cumulative explained variance reaches **~95%** (e.g., 2â€“3 components depending on the elbow point in the plot).\n"
      ],
      "metadata": {
        "id": "9RVLe23nXGz3"
      }
    }
  ]
}